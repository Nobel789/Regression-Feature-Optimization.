# Regression-Feature-Optimization.
The notebooks focus heavily on improving model performance through statistical methods like Backward Elimination, Multicollinearity removal, and Polynomial Regression.


Regression Analysis & Feature OptimizationThis repository demonstrates advanced techniques in regression modeling, specifically focusing on feature selection strategies to build parsimonious and accurate models.üß† Key Statistical Techniques1. Backward Elimination & Feature SelectionMethod: Implemented Stepwise Regression (Backward Elimination) to iteratively remove features with $p$-values $> 0.05$.Result: Optimized a Housing Price model by reducing dimensions while maintaining an Adjusted $R^2$ of 0.905.Tools: statsmodels.api (OLS), VIF (Variance Inflation Factor) for detecting multicollinearity.2. Polynomial RegressionComparison: Evaluated Linear vs. Polynomial (Degree 2) regression on Advertising Sales data.Performance: Improved model accuracy ($R^2$) from 0.92 (Linear) to 0.99 (Polynomial) by capturing non-linear market trends.3. Outlier Detection (EDA)Visual Diagnostics: utilized Box Plots and Bar Charts to identify and quantify outliers in Invoice Data across different regions.üõ†Ô∏è Libraries Usedstatsmodels: For OLS regression and statistical tests.scikit-learn: For LinearRegression, PolynomialFeatures, and metrics ($R^2$).matplotlib / seaborn: For visualizing regression lines and box plots.
